{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-mjjBImJF2L"
      },
      "source": [
        "#**Q&A over the Code Base to Understand How it Works**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osDa8_mzJk2C"
      },
      "source": [
        "#**Step 01: Install All the Required Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by9_WiR7IhFv",
        "outputId": "667cf6a6-8830-46bc-ba2d-b83391143a85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.4.10-py3-none-any.whl (422 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.4/422.4 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.292-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.12)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n",
            "  Downloading langsmith-0.0.37-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Collecting huggingface_hub<0.17,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=49d99ca77ed7996856cc76fc48d1cbd4dc34e305755b9c463ca9ca93e209db5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, websockets, uvloop, python-dotenv, pulsar-client, overrides, mypy-extensions, marshmallow, humanfriendly, httptools, h11, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, langsmith, huggingface_hub, coloredlogs, tokenizers, openai, onnxruntime, fastapi, dataclasses-json, langchain, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.10 coloredlogs-15.0.1 dataclasses-json-0.5.14 fastapi-0.99.1 h11-0.14.0 httptools-0.6.0 huggingface_hub-0.16.4 humanfriendly-10.0 langchain-0.0.292 langsmith-0.0.37 marshmallow-3.20.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.15.1 openai-0.28.0 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tiktoken-0.5.1 tokenizers-0.14.0 typing-inspect-0.9.0 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.20.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install openai tiktoken chromadb langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QryMudLjKy-E",
        "outputId": "3d71706a-43d0-455c-fffa-17c2674ca202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting GitPython\n",
            "  Downloading GitPython-3.1.36-py3-none-any.whl (189 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/189.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/189.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.5/189.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, GitPython\n",
            "Successfully installed GitPython-3.1.36 gitdb-4.0.10 smmap-5.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install GitPython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqaNhoWsJ1PF"
      },
      "source": [
        "#**Step 02: Import All the Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fz1JKOWtJtuZ"
      },
      "outputs": [],
      "source": [
        "# To clone the github repo in the Google Colab Notebook\n",
        "from git import Repo\n",
        "from langchain.text_splitter import Language\n",
        "# To load the files\n",
        "from langchain.document_loaders.generic import GenericLoader\n",
        "# In the Language Parser we will pass the extension of the programming language we are working with,\n",
        "# Here we are working with Pyton programming language so we will add Python, if we are working with\n",
        "#Java, C++ or any other programming language we can add that extension as well\n",
        "from langchain.document_loaders.parsers import LanguageParser\n",
        "# After loading all the code in a variable we will split the code into small chunks using Recursive Character Text Splitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "import sys\n",
        "# We will download OpenAI Embeddings\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "# We will save the embeddings in the Chroma Vector Store\n",
        "from langchain.vectorstores import Chroma\n",
        "# We will create a ChatOpenAI wrapper, to chat with our document\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "# We can add memory to our Q/A chain using Conversation Summary Memory, it will look at the historical conversation\n",
        "# Computer the Summary and add that to a subsequent conversation\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_UCYeVxLGtr"
      },
      "source": [
        "#**Step 03: Clone the LangChain Github Repo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LyBOoIDlLVBo"
      },
      "outputs": [],
      "source": [
        "!mkdir test_repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rsBti-sGKmVV"
      },
      "outputs": [],
      "source": [
        "repo_path = \"/content/test_repo\"\n",
        "\n",
        "repo = Repo.clone_from(\"https://github.com/hwchase17/langchain\", to_path=repo_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPbmZY-eL51T"
      },
      "source": [
        "#**Step 04: Load the Data from the Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CshMDoa7LOfD"
      },
      "outputs": [],
      "source": [
        "loader = GenericLoader.from_filesystem(repo_path+'/libs/langchain/langchain',\n",
        "                                        glob = \"**/*\",\n",
        "                                       suffixes=[\".py\"],\n",
        "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T93X1D_qMJos"
      },
      "outputs": [],
      "source": [
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL4N0gRTMigP",
        "outputId": "71093572-9cb7-4b40-94f0-ba71b10836cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1428"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-dHilmLMk7-",
        "outputId": "544e81d3-f785-4b68-b308-9bda86732ebb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='\"\"\"Experiment with different models.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import List, Optional, Sequence\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.prompts.prompt import PromptTemplate\\nfrom langchain.utils.input import get_color_mapping, print_text\\n\\n\\nclass ModelLaboratory:\\n    \"\"\"Experiment with different models.\"\"\"\\n\\n    def __init__(self, chains: Sequence[Chain], names: Optional[List[str]] = None):\\n        \"\"\"Initialize with chains to experiment with.\\n\\n        Args:\\n            chains: list of chains to experiment with.\\n        \"\"\"\\n        for chain in chains:\\n            if not isinstance(chain, Chain):\\n                raise ValueError(\\n                    \"ModelLaboratory should now be initialized with Chains. \"\\n                    \"If you want to initialize with LLMs, use the `from_llms` method \"\\n                    \"instead (`ModelLaboratory.from_llms(...)`)\"\\n                )\\n            if len(chain.input_keys) != 1:\\n                raise ValueError(\\n                    \"Currently only support chains with one input variable, \"\\n                    f\"got {chain.input_keys}\"\\n                )\\n            if len(chain.output_keys) != 1:\\n                raise ValueError(\\n                    \"Currently only support chains with one output variable, \"\\n                    f\"got {chain.output_keys}\"\\n                )\\n        if names is not None:\\n            if len(names) != len(chains):\\n                raise ValueError(\"Length of chains does not match length of names.\")\\n        self.chains = chains\\n        chain_range = [str(i) for i in range(len(self.chains))]\\n        self.chain_colors = get_color_mapping(chain_range)\\n        self.names = names\\n\\n    @classmethod\\n    def from_llms(\\n        cls, llms: List[BaseLLM], prompt: Optional[PromptTemplate] = None\\n    ) -> ModelLaboratory:\\n        \"\"\"Initialize with LLMs to experiment with and optional prompt.\\n\\n        Args:\\n            llms: list of LLMs to experiment with\\n            prompt: Optional prompt to use to prompt the LLMs. Defaults to None.\\n                If a prompt was provided, it should only have one input variable.\\n        \"\"\"\\n        if prompt is None:\\n            prompt = PromptTemplate(input_variables=[\"_input\"], template=\"{_input}\")\\n        chains = [LLMChain(llm=llm, prompt=prompt) for llm in llms]\\n        names = [str(llm) for llm in llms]\\n        return cls(chains, names=names)\\n\\n    def compare(self, text: str) -> None:\\n        \"\"\"Compare model outputs on an input text.\\n\\n        If a prompt was provided with starting the laboratory, then this text will be\\n        fed into the prompt. If no prompt was provided, then the input text is the\\n        entire prompt.\\n\\n        Args:\\n            text: input text to run all models on.\\n        \"\"\"\\n        print(f\"\\\\033[1mInput:\\\\033[0m\\\\n{text}\\\\n\")\\n        for i, chain in enumerate(self.chains):\\n            if self.names is not None:\\n                name = self.names[i]\\n            else:\\n                name = str(chain)\\n            print_text(name, end=\"\\\\n\")\\n            output = chain.run(text)\\n            print_text(output, color=self.chain_colors[str(i)], end=\"\\\\n\\\\n\")\\n', metadata={'source': '/content/test_repo/libs/langchain/langchain/model_laboratory.py', 'language': <Language.PYTHON: 'python'>})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO6IJH0sNHYr"
      },
      "source": [
        "#**Step 05: Split the Document into Chunks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rDDUMNAcMmWC"
      },
      "outputs": [],
      "source": [
        "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
        "                                                             chunk_size = 2000,\n",
        "                                                             chunk_overlap = 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ol6eRiuNNplM"
      },
      "outputs": [],
      "source": [
        "texts = documents_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC85oDiENzjD",
        "outputId": "ae46e907-5ae5-499f-a9f7-b1f7e0ab6da4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4395"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKRgzXdqPeDZ"
      },
      "source": [
        "#**Step 06: Download the OpenAI Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GoE8TpknN8QG"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zvoxmidZQmBi"
      },
      "outputs": [],
      "source": [
        "embeddings=OpenAIEmbeddings(disallowed_special=())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8pAFIKeSf4C"
      },
      "source": [
        "#**Step 07: Setting Up Chroma as out Vector Database**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN-wgZFXSv-n"
      },
      "source": [
        " Converting the Document Chunks into Embeddings and save them into a Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8F92caqIRy4C"
      },
      "outputs": [],
      "source": [
        "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r1F5SdsbSUOC"
      },
      "outputs": [],
      "source": [
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIxIspWaTuZJ"
      },
      "source": [
        "#**Step 08: Creating an OpenAI Model Warpper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OIZaATJHS0b_"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HthbColvWX0u",
        "outputId": "3423c6b6-e6b0-476c-a63f-0be685476a32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4', temperature=0.7, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NCW-X_W9T4UC"
      },
      "outputs": [],
      "source": [
        "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "p8Ww86NGUMXi"
      },
      "outputs": [],
      "source": [
        "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eMoZfLoVNV-"
      },
      "source": [
        "#**Question and Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rbGp_zcxU76v"
      },
      "outputs": [],
      "source": [
        "question = \"How i can initialize the ReAct Agent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5raZU1AhWMNx"
      },
      "outputs": [],
      "source": [
        "result = qa(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4v222rJWONv",
        "outputId": "4da22623-7a62-4620-fc47-bcecf661e3c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'question': 'How i can initialize the ReAct Agent', 'chat_history': [SystemMessage(content='', additional_kwargs={})], 'answer': 'The ReAct Agent is initialized through the following steps:\\n\\n1. Define the `output_parser`: The `output_parser` is set to `ReActOutputParser` by default. This is used to parse the output of the agent\\'s actions.\\n\\n2. Set the agent type: The `_agent_type` property is set to `REACT_DOCSTORE`. This identifies the type of agent.\\n\\n3. Define the prompt creation method: The `create_prompt` method is used to set the default prompt for the agent. This is set to `WIKI_PROMPT`.\\n\\n4. Validate the tools: The `_validate_tools` method is used to ensure that exactly two tools are being used, named \"Lookup\" and \"Search\". If this is not the case, a `ValueError` is raised.\\n\\n5. Set observation prefix: The `observation_prefix` is set to \"Observation: \". This is used to prepend the observations made by the agent during its actions.\\n\\n6. Set the stop list: The `_stop` property is set to a list containing \"\\\\nObservation:\". This defines when the agent should stop its actions.\\n\\n7. Set LLM prefix: The `llm_prefix` is set to \"Thought:\". This is used to prepend the thoughts of the agent during its actions. \\n\\nThis is the basic initialization process for the ReAct Agent. Other properties and methods may be defined as per the requirements of a specific application.'}\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCdnToQ_WQMA",
        "outputId": "c74721de-e567-4c41-a173-e73500e1e982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The ReAct Agent is initialized through the following steps:\n",
            "\n",
            "1. Define the `output_parser`: The `output_parser` is set to `ReActOutputParser` by default. This is used to parse the output of the agent's actions.\n",
            "\n",
            "2. Set the agent type: The `_agent_type` property is set to `REACT_DOCSTORE`. This identifies the type of agent.\n",
            "\n",
            "3. Define the prompt creation method: The `create_prompt` method is used to set the default prompt for the agent. This is set to `WIKI_PROMPT`.\n",
            "\n",
            "4. Validate the tools: The `_validate_tools` method is used to ensure that exactly two tools are being used, named \"Lookup\" and \"Search\". If this is not the case, a `ValueError` is raised.\n",
            "\n",
            "5. Set observation prefix: The `observation_prefix` is set to \"Observation: \". This is used to prepend the observations made by the agent during its actions.\n",
            "\n",
            "6. Set the stop list: The `_stop` property is set to a list containing \"\\nObservation:\". This defines when the agent should stop its actions.\n",
            "\n",
            "7. Set LLM prefix: The `llm_prefix` is set to \"Thought:\". This is used to prepend the thoughts of the agent during its actions. \n",
            "\n",
            "This is the basic initialization process for the ReAct Agent. Other properties and methods may be defined as per the requirements of a specific application.\n"
          ]
        }
      ],
      "source": [
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0tsFAgTAWS-b"
      },
      "outputs": [],
      "source": [
        "question = \"What is the class hierarchy?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mJH5UO8XRHL5"
      },
      "outputs": [],
      "source": [
        "result = qa(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRaO3864RJl9",
        "outputId": "9d78f5f3-e914-4aba-b387-c8205c3ee2bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The class hierarchy for **Memory** is:\n",
            "\n",
            "    BaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory\n",
            "\n",
            "The class hierarchy for **Chat Message History** is:\n",
            "\n",
            "    BaseChatMessageHistory --> <name>ChatMessageHistory  # Example: ZepChatMessageHistory\n"
          ]
        }
      ],
      "source": [
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOotxBCaRNS3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
